{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Python\n",
    "\n",
    "\n",
    "For this course, we are going to use Jupyter notebook as our environment for developing Python code.\n",
    "refer to https://jupyter.readthedocs.io/en/latest/content-quickstart.html on the instructions how to install it, the easiest way is to install from Anaconda (https://www.anaconda.com/download/) website, make sure you install (Version 4.4.0, Release Date: May 31, 2017) with Python 3.6.\n",
    "\n",
    "Also, it is good for the students who are not familiar with python (or they need a quick refreshment) to follow Jim Bagrow tutorial http://bagrow.com/ds1/whirlwindtourpython/00-Title.html. \n",
    "\n",
    "All the assignments to be written in Python 3.6 and can be run using Jupyter on one of the following Internet browsers (Chrome, Safari or Firefox), these are the browsers that officially supported by jupyter.\n",
    "\n",
    "<u> Note: for this assignment, submit your local copy of this page, running on IPython. Submit the file to Blackboard under Assignment1 using this file format:</u> <b>Yourfirstname_lastname_Assignment4.ipynb</b> \n",
    "\n",
    "#### <b>Deadline</b>: <u>Monday, Dec-04-2017 11:59 PM.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4\n",
    "\n",
    "In assignment 4 use tensor flow to build and train neural network architectures in part1 and part2.\n",
    "Hint: Refer to the MLP.ipynb from the shared documents on the black borad as an example to build and train a neural network using tensor flow from scratch.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1\n",
    "In this part you are going to train a neural network on notMNIST dataset. The notMNIST dataset is a image recognition dataset of font glypyhs for the letters A through J useful with simple neural networks. It is quite similar to the classic MNIST dataset of handwritten digits 0 through 9. to make easy for you. the code below will load the notMNIST dataset into train, validation and testing arrays to use them during the training of the network.  \n",
    "you need to download notMNIST.data from this link ...\n",
    "\n",
    "https://drive.google.com/file/d/1ablp83xroWod-Mfr0dGlVM49YXNm32l7/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import cPickle as loader\n",
    "from six.moves import range\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "data_file = 'notMNIST.data'\n",
    "\n",
    "with open(data_file, 'rb') as f:\n",
    "  data = loader.load(f)\n",
    "  train_dataset = data['train_dataset']\n",
    "  train_labels = data['train_labels']\n",
    "  valid_dataset = data['valid_dataset']\n",
    "  valid_labels = data['valid_labels']\n",
    "  test_dataset = data['test_dataset']\n",
    "  test_labels = data['test_labels']\n",
    "  del data \n",
    "\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Convince yourself about the data by showing few  images and printing the size of train/test/validation data arrays.[10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1443bfcf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The letter in this image is E \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16ac6afd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The letter in this image is J \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16ab56978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The letter in this image is G \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFj+RUyx+IqdY/EROsfiJnGLxEzn1f2wPzZwb8hszAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16b1dcda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The letter in this image is C \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noBU+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x143ce5f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The letter in this image is H \n",
      "\n",
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "#show the fisrt five images\n",
    "for i in range(5):\n",
    "    image= train_dataset[i]\n",
    "    plt.imshow(image, cmap=plt.cm.gray)\n",
    "    plt.show()\n",
    "    print('The letter in this image is', chr(train_labels[i]+ ord('A')), '\\n')\n",
    "\n",
    "#print the size of train/test/validation data arrays\n",
    "#print(type(train_dataset))\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b.  Reformat data to shape that's make it easy to train neural network models:\n",
    "-\t Reshape the data to a flat matrix.[5 points]. Hint: For example convert training dataset to have (200000, 784) shape, do the same for the rest of the arrays.\n",
    "-\t Reformat the output to 1-hot encodings. [5 points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset (200000, 784)\n",
      "Validation dataset (10000, 784)\n",
      "Test dataset (10000, 784)\n",
      "Training labels (200000, 10)\n",
      "Validation labels (10000, 10)\n",
      "Test labels (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "#reshape dataset to a flat matrix\n",
    "train_dataset_flat = np.reshape(train_dataset, (200000,784))\n",
    "valid_dataset_flat = np.reshape(valid_dataset, (10000,784))\n",
    "test_dataset_flat = np.reshape(test_dataset, (10000,784))\n",
    "print('Training dataset', train_dataset_flat.shape)\n",
    "print('Validation dataset', valid_dataset_flat.shape)\n",
    "print('Test dataset', test_dataset_flat.shape)\n",
    "\n",
    "#function to reformat the output to 1-hot encodings\n",
    "def onehot_encoding(dataset):\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    dataset = dataset.reshape(len(dataset), 1)\n",
    "    dataset_output = onehot_encoder.fit_transform(dataset)\n",
    "    return dataset_output\n",
    "\n",
    "train_labels_encoded = onehot_encoding(train_labels)\n",
    "valid_labels_encoded = onehot_encoding(valid_labels)\n",
    "test_labels_encoded = onehot_encoding(test_labels)\n",
    "print('Training labels', train_labels_encoded.shape)\n",
    "print('Validation labels', valid_labels_encoded.shape)\n",
    "print('Test labels', test_labels_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Refer to the neural network lecture to answer the following questions (no need to code or to show experimental results for this question) [15 points]:\n",
    "- the best activation function.\n",
    "- the best initialization.\n",
    "- Best gradient descent update learning rule. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The best activation function is ReLU.\n",
    "\n",
    "The best initialization is \n",
    "W = np.random.randn(fan_in, fan_out)/np.sqrt(fan_in/2) introduced by He et al. in 2015, which is recommended with ReLU.\n",
    "\n",
    "Best gradient descent update learning rule is Adam.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Train a multi-layer neural network of one hidden layer neural network using the best: activation, initialization and gradient descent learning rule that you found in the previous question. \n",
    "Train for 15 epochs with a  batch size of 128 and pick the model that has less validation error and report the testing error for that model. Use the accuracy metric to measure the performance. [35 points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the network toplogies, one hidden neural network with input diminsion of 784 and output diminsion of 10\n",
    "n_hidden = 256 \n",
    "n_input    = 784 \n",
    "n_classes  = 10  \n",
    "\n",
    "# placeholder for input and output, None: means it could be of any diminsion\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# network parameters \n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "weights = {\n",
    "    'h': tf.Variable(initializer([n_input, n_hidden])),\n",
    "    'out': tf.Variable(initializer([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b': tf.Variable(initializer([n_hidden])),\n",
    "    'out': tf.Variable(initializer([n_classes]))\n",
    "}\n",
    "# weights = {\n",
    "#     'h': tf.get_variable('1',shape=[n_input, n_hidden],initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "#     'out': tf.get_variable('2',shape=[n_hidden, n_classes],initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "# }\n",
    "# biases = {\n",
    "#     'b': tf.get_variable('3',shape=[n_hidden],initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "#     'out': tf.get_variable('4',shape=[n_classes],initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define ReLU as active function\n",
    "def multilayer_perceptron(_X, _weights, _biases):\n",
    "    layer =  tf.nn.relu(tf.add(tf.matmul(_X, _weights['h']), _biases['b']))\n",
    "    return (tf.matmul(layer, _weights['out']) + _biases['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the network graph\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "# use softmax cross entropy as the loss(cost) function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred, labels=y)) \n",
    "# define optimizer\n",
    "optm = tf.train.AdamOptimizer().minimize(cost) \n",
    "# measuring the accuracy\n",
    "corr = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accr = tf.reduce_mean(tf.cast(corr, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready to go ...\n",
      "Epoch: 003/015 cost: 0.301535394\n",
      "Train Accuracy: 0.882\n",
      "Valid Accuracy: 0.887\n",
      "Epoch: 007/015 cost: 0.231270141\n",
      "Train Accuracy: 0.921\n",
      "Valid Accuracy: 0.892\n",
      "Epoch: 011/015 cost: 0.185920580\n",
      "Train Accuracy: 0.969\n",
      "Valid Accuracy: 0.891\n",
      "optimization finished \n",
      "\n",
      "The best valid Accuracy: 0.892\n",
      "The test Accuracy for best valid Accuracy model: 0.949\n",
      "The test error for best valid Accuracy model: 0.051\n"
     ]
    }
   ],
   "source": [
    "# Training params\n",
    "training_epochs = 15\n",
    "batch_size      = 128 # use mini-batch\n",
    "display_step    = 4 # display results every 4 epochs\n",
    "valid_acc_best = 0.\n",
    "test_acc_betst =0.\n",
    "# launch the graph using Session()\n",
    "sess = tf.Session()\n",
    "# initilize all the graph parameters.\n",
    "# run with the initiales\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print (\"ready to go ...\")\n",
    "# optimization code, we used here stochastic gradient descent SGD.\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(train_dataset_flat.shape[0]/batch_size)\n",
    "    # iterate\n",
    "    for i in range(total_batch):\n",
    "        batch_xs = train_dataset_flat[batch_size*i:batch_size*(i+1)-1]\n",
    "        #print(batch_xs.shape)\n",
    "        batch_ys = train_labels_encoded[batch_size*i:batch_size*(i+1)-1]\n",
    "        #print(batch_ys.shape)\n",
    "        feeds = {x: batch_xs, y: batch_ys}\n",
    "        sess.run(optm, feed_dict=feeds)\n",
    "        avg_cost += sess.run(cost, feed_dict=feeds) # to calculate the cost\n",
    "    avg_cost = avg_cost / total_batch\n",
    "    # display\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        feeds = {x: batch_xs, y: batch_ys}\n",
    "        train_acc = sess.run(accr, feed_dict=feeds)    \n",
    "        feeds = {x: valid_dataset_flat, y: valid_labels_encoded}\n",
    "        valid_acc = sess.run(accr, feed_dict=feeds)\n",
    "        feeds = {x: test_dataset_flat, y: test_labels_encoded}\n",
    "        test_acc = sess.run(accr, feed_dict=feeds)\n",
    "        if valid_acc_best <= valid_acc:\n",
    "            valid_acc_best = valid_acc\n",
    "            test_acc_best = test_acc\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print (\"Epoch: %03d/%03d cost: %.9f\" % (epoch, training_epochs, avg_cost))\n",
    "            print (\"Train Accuracy: %.3f\" % (train_acc))\n",
    "            print (\"Valid Accuracy: %.3f\" % (valid_acc))\n",
    "\n",
    "print (\"optimization finished \\n\")\n",
    "print (\"The best valid Accuracy: %.3f\" % (valid_acc_best))\n",
    "print (\"The test Accuracy for best valid Accuracy model: %.3f\" % (test_acc_best))\n",
    "print (\"The test error for best valid Accuracy model: %.3f\" % (1-test_acc_best))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. (This part for graduate students only) Which regularization method is the best to use, L2, dropout or Batch Normalization (justify your answer experimentally). does the batch normalization help in converging the network faster ? [30 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready to go ...\n",
      "***************************** L2 Regularization *********************************************\n",
      "Epoch: 003/015 cost: 0.710508094\n",
      "Train Accuracy: 0.819\n",
      "Valid Accuracy: 0.836\n",
      "Epoch: 007/015 cost: 0.702673132\n",
      "Train Accuracy: 0.819\n",
      "Valid Accuracy: 0.838\n",
      "Epoch: 011/015 cost: 0.700404129\n",
      "Train Accuracy: 0.819\n",
      "Valid Accuracy: 0.838\n",
      "optimization finished \n",
      "\n",
      "The best valid Accuracy: 0.839\n",
      "The test Accuracy for best valid Accuracy model: 0.906\n",
      "The test error for best valid Accuracy model: 0.094\n"
     ]
    }
   ],
   "source": [
    "#L2 Regularization\n",
    "beta = 0.01\n",
    "cost_l2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred, labels=y) + beta*tf.nn.l2_loss(weights['h']) +beta*tf.nn.l2_loss(weights['out']))\n",
    "optm_l2 = tf.train.AdamOptimizer().minimize(cost_l2) \n",
    "\n",
    "# Training params\n",
    "training_epochs = 15\n",
    "batch_size      = 128 # use mini-batch\n",
    "display_step    = 4 # display results every 4 epochs\n",
    "valid_acc_best = 0.\n",
    "test_acc_best = 0.\n",
    "# launch the graph using Session()\n",
    "sess = tf.Session()\n",
    "# initilize all the graph parameters.\n",
    "# run with the initiales\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print (\"ready to go ...\")\n",
    "print('***************************** L2 Regularization *********************************************')\n",
    "\n",
    "# optimization code, we used here stochastic gradient descent SGD.\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(train_dataset_flat.shape[0]/batch_size)\n",
    "    # iterate\n",
    "    for i in range(total_batch):\n",
    "        batch_xs = train_dataset_flat[batch_size*i:batch_size*(i+1)-1]\n",
    "        #print(batch_xs.shape)\n",
    "        batch_ys = train_labels_encoded[batch_size*i:batch_size*(i+1)-1]\n",
    "        #print(batch_ys.shape)\n",
    "        feeds = {x: batch_xs, y: batch_ys}\n",
    "        sess.run(optm_l2, feed_dict=feeds)\n",
    "        avg_cost += sess.run(cost_l2, feed_dict=feeds) # to calculate the cost\n",
    "    avg_cost = avg_cost / total_batch\n",
    "    # display\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        feeds = {x: batch_xs, y: batch_ys}\n",
    "        train_acc = sess.run(accr, feed_dict=feeds)    \n",
    "        feeds = {x: valid_dataset_flat, y: valid_labels_encoded}\n",
    "        valid_acc = sess.run(accr, feed_dict=feeds)\n",
    "        feeds = {x: test_dataset_flat, y: test_labels_encoded}\n",
    "        test_acc = sess.run(accr, feed_dict=feeds)\n",
    "        if valid_acc_best <= valid_acc:\n",
    "            valid_acc_best = valid_acc\n",
    "            test_acc_best = test_acc\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print (\"Epoch: %03d/%03d cost: %.9f\" % (epoch, training_epochs, avg_cost))\n",
    "            print (\"Train Accuracy: %.3f\" % (train_acc))\n",
    "            print (\"Valid Accuracy: %.3f\" % (valid_acc))\n",
    "    \n",
    "print (\"optimization finished \\n\")\n",
    "print (\"The best valid Accuracy: %.3f\" % (valid_acc_best))\n",
    "print (\"The test Accuracy for best valid Accuracy model: %.3f\" % (test_acc_best))\n",
    "print (\"The test error for best valid Accuracy model: %.3f\" % (1-test_acc_best))\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready to go ...\n",
      "***************************** Dropout *********************************************\n",
      "Epoch: 003/015 cost: 0.464916616\n",
      "Train Accuracy: 0.843\n",
      "Valid Accuracy: 0.878\n",
      "Epoch: 007/015 cost: 0.429267553\n",
      "Train Accuracy: 0.843\n",
      "Valid Accuracy: 0.882\n",
      "Epoch: 011/015 cost: 0.405968954\n",
      "Train Accuracy: 0.850\n",
      "Valid Accuracy: 0.886\n",
      "optimization finished \n",
      "\n",
      "The best valid Accuracy: 0.889\n",
      "The test Accuracy for best valid Accuracy model: 0.948\n",
      "The test error for best valid Accuracy model: 0.052\n"
     ]
    }
   ],
   "source": [
    "#Dropout \n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "# Training computation.\n",
    "logits_1 = tf.matmul(x, weights['h']) + biases['b']\n",
    "relu_layer= tf.nn.relu(logits_1)\n",
    "# Dropout on hidden layer: RELU layer\n",
    "relu_layer_dropout = tf.nn.dropout(relu_layer, keep_prob)\n",
    "logits_2 = tf.matmul(relu_layer_dropout, weights['out']) + biases['out']\n",
    "# Normal loss function\n",
    "cost_dropout = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits_2, labels=y))\n",
    "optm_dropout = tf.train.AdamOptimizer().minimize(cost_dropout) \n",
    "\n",
    "corr_dropout = tf.equal(tf.argmax(logits_2, 1), tf.argmax(y, 1))\n",
    "accr_dropout = tf.reduce_mean(tf.cast(corr_dropout, \"float\"))\n",
    "\n",
    "print(\"ready to go ...\")\n",
    "print('***************************** Dropout *********************************************')\n",
    "\n",
    "# Training params\n",
    "training_epochs = 15\n",
    "batch_size      = 128 # use mini-batch\n",
    "display_step    = 4 # display results every 4 epochs\n",
    "valid_acc_best = 0.\n",
    "test_acc_best = 0.\n",
    "# launch the graph using Session()\n",
    "# initilize all the graph parameters.\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# optimization code, we used here stochastic gradient descent SGD.\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(train_dataset_flat.shape[0]/batch_size)\n",
    "    # iterate\n",
    "    for i in range(total_batch):\n",
    "        batch_xs = train_dataset_flat[batch_size*i:batch_size*(i+1)-1]\n",
    "        #print(batch_xs.shape)\n",
    "        batch_ys = train_labels_encoded[batch_size*i:batch_size*(i+1)-1]\n",
    "        #print(batch_ys.shape)\n",
    "        feeds = {x: batch_xs, y: batch_ys, keep_prob: 0.5}\n",
    "        sess.run(optm_dropout, feed_dict=feeds)\n",
    "        avg_cost += sess.run(cost_dropout, feed_dict=feeds) # to calculate the cost\n",
    "    avg_cost = avg_cost / total_batch\n",
    "    # display\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        feeds = {x: batch_xs, y: batch_ys, keep_prob: 0.5}\n",
    "        train_acc = sess.run(accr_dropout, feed_dict=feeds)    \n",
    "        feeds = {x: valid_dataset_flat, y: valid_labels_encoded, keep_prob: 1}\n",
    "        valid_acc = sess.run(accr_dropout, feed_dict=feeds)\n",
    "        feeds = {x: test_dataset_flat, y: test_labels_encoded, keep_prob: 1}\n",
    "        test_acc = sess.run(accr_dropout, feed_dict=feeds)\n",
    "    if valid_acc_best <= valid_acc:\n",
    "            valid_acc_best = valid_acc\n",
    "            test_acc_best = test_acc\n",
    "    if (epoch+1) % display_step == 0:\n",
    "            print (\"Epoch: %03d/%03d cost: %.9f\" % (epoch, training_epochs, avg_cost))\n",
    "            print (\"Train Accuracy: %.3f\" % (train_acc))\n",
    "            print (\"Valid Accuracy: %.3f\" % (valid_acc))\n",
    "\n",
    "print (\"optimization finished \\n\")\n",
    "print (\"The best valid Accuracy: %.3f\" % (valid_acc_best))\n",
    "print (\"The test Accuracy for best valid Accuracy model: %.3f\" % (test_acc_best))\n",
    "print (\"The test error for best valid Accuracy model: %.3f\" % (1-test_acc_best))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready to go ...\n",
      "***************************** Batch Normalization *********************************************\n",
      "Epoch: 003/015 cost: 0.299544014\n",
      "Train Accuracy: 0.874\n",
      "Valid Accuracy: 0.888\n",
      "Epoch: 007/015 cost: 0.221601382\n",
      "Train Accuracy: 0.921\n",
      "Valid Accuracy: 0.889\n",
      "Epoch: 011/015 cost: 0.170271823\n",
      "Train Accuracy: 0.937\n",
      "Valid Accuracy: 0.884\n",
      "optimization finished \n",
      "\n",
      "The best valid Accuracy: 0.890\n",
      "The test Accuracy for best valid Accuracy model: 0.949\n",
      "The test error for best valid Accuracy model: 0.051\n"
     ]
    }
   ],
   "source": [
    "#Batch Normalization\n",
    "# Small epsilon value for the BN transform\n",
    "epsilon = 1e-3\n",
    "# Note that pre-batch normalization bias is ommitted.\n",
    "z1_BN = tf.matmul(x, weights['h'])\n",
    "#print(z1_BN)\n",
    "# Calculate batch mean and variance\n",
    "batch_mean1, batch_var1 = tf.nn.moments(z1_BN,[0])\n",
    "#print(batch_mean1)\n",
    "# Apply the initial batch normalizing transform\n",
    "z1_hat = (z1_BN - batch_mean1) / tf.sqrt(batch_var1 + epsilon)\n",
    "# Create two new parameters, scale and beta (shift)\n",
    "scale1 = tf.Variable(tf.ones([256]))\n",
    "beta1 = tf.Variable(tf.zeros([256]))\n",
    "# Scale and shift to obtain the final output of the batch normalization\n",
    "BN2 = tf.nn.batch_normalization(z1_BN,batch_mean1,batch_var1,beta1,scale1,epsilon)\n",
    "l1_BN = tf.nn.relu(BN2)\n",
    "l2 = tf.matmul(l1_BN, weights['out']) + biases['out']\n",
    "# Normal loss function\n",
    "cost_BN = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = l2, labels=y))\n",
    "optm_BN = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost_BN) \n",
    "\n",
    "corr_BN = tf.equal(tf.argmax(l2, 1), tf.argmax(y, 1))\n",
    "accr_BN = tf.reduce_mean(tf.cast(corr_BN, \"float\"))\n",
    "\n",
    "# Training params\n",
    "training_epochs = 15\n",
    "batch_size      = 128 # use mini-batch\n",
    "display_step    = 4 # display results every 4 epochs\n",
    "valid_acc_best = 0.\n",
    "test_acc_best = 0.\n",
    "# initilize all the graph parameters.\n",
    "# launch the graph using Session()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print (\"ready to go ...\")\n",
    "print('***************************** Batch Normalization *********************************************')\n",
    "\n",
    "# optimization code, we used here stochastic gradient descent SGD.\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(train_dataset_flat.shape[0]/batch_size)\n",
    "    # iterate\n",
    "    for i in range(total_batch):\n",
    "        batch_xs = train_dataset_flat[batch_size*i:batch_size*(i+1)-1]\n",
    "        #print(batch_xs.shape)\n",
    "        batch_ys = train_labels_encoded[batch_size*i:batch_size*(i+1)-1]\n",
    "        #print(batch_ys.shape)\n",
    "        feeds = {x: batch_xs, y: batch_ys}\n",
    "        sess.run(optm_BN, feed_dict=feeds)\n",
    "        avg_cost += sess.run(cost_BN, feed_dict=feeds) # to calculate the cost\n",
    "    avg_cost = avg_cost / total_batch\n",
    "    # display\n",
    "    # display\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        feeds = {x: batch_xs, y: batch_ys}\n",
    "        train_acc = sess.run(accr_BN, feed_dict=feeds)    \n",
    "        feeds = {x: valid_dataset_flat, y: valid_labels_encoded}\n",
    "        valid_acc = sess.run(accr_BN, feed_dict=feeds)\n",
    "        feeds = {x: test_dataset_flat, y: test_labels_encoded}\n",
    "        test_acc = sess.run(accr_BN, feed_dict=feeds)\n",
    "        if valid_acc_best <= valid_acc:\n",
    "            valid_acc_best = valid_acc\n",
    "            test_acc_best = test_acc\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print (\"Epoch: %03d/%03d cost: %.9f\" % (epoch, training_epochs, avg_cost))\n",
    "            print (\"Train Accuracy: %.3f\" % (train_acc))\n",
    "            print (\"Valid Accuracy: %.3f\" % (valid_acc))\n",
    "\n",
    "print (\"optimization finished \\n\")\n",
    "print (\"The best valid Accuracy: %.3f\" % (valid_acc_best))\n",
    "print (\"The test Accuracy for best valid Accuracy model: %.3f\" % (test_acc_best))\n",
    "print (\"The test error for best valid Accuracy model: %.3f\" % (1-test_acc_best))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the experimental results, batch normalization is best to use, which shows the highest test accuracy. There are two ways that batch normalization potentially helps, one is faster learning and the other one is higher overall accuracy. Thus, batch normalization allows to use a higher learning rate, potentially proving a way to converge the network faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This whole part is for graduate students only) In this part you are going to train a convolutional neural network on notMNIST dataset, refer to part1 to use same training, validation and testing datasets. \n",
    "\n",
    "Start by reading the Deep MNIST for Experts tutorial on the tensorflow website (https://www.tensorflow.org/get_started/mnist/pros). \n",
    "Go through the examples provided in this tutorial. \n",
    "\n",
    "\n",
    "in here we are going to build a different network. Itd be helpful to quickly go through the examples provided in the tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Define the model as indicated in the code below. The model is defined as follows:\n",
    "\n",
    "- An input that is 728 dimensional vector.\n",
    "- Reshape the input as 28x28x1 images (only 1 because they are grey scale)\n",
    "- A convolutional layer with 25 filters of shape 12x12x1 and a ReLU non-linearity (with stride (2, 2) and no padding)\n",
    "- A convolutional layer with 64 filters of shape 5x5x25 and a ReLU non-linearity (with stride (1, 2) and padding to maintain size)\n",
    "- A max_pooling layer of shape 2x2\n",
    "- A fully connected layer taking all the outputs of the max_pooling layer to 1024 units and ReLU nonlinearity\n",
    "- A fully connected layer taking 1024 units to 10 no activation function (the softmax non-linearity will be included in the loss function rather than in the model) [15 points]\n",
    "\n",
    "Hint: start from known architecture then modify the code to match the numbers listed above, you might need to have flat layer that flatten max pool layer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "#initialize weight\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "#initialize bias\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d_1(x, W, strides, padding):\n",
    "  return tf.nn.conv2d(x, W, strides=strides, padding=padding)\n",
    "\n",
    "def max_pool_2x2_1(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#builds the graph for a deep net\n",
    "def deepnn(x):\n",
    "  # Reshape to use within a convolutional neural net.\n",
    "  # Last dimension is for \"features\" - there is only one here, since images are\n",
    "  # grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n",
    "  with tf.name_scope('reshape'):\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "  # First convolutional layer - maps one grayscale image to 25 feature maps.\n",
    "  with tf.name_scope('conv1'):\n",
    "    W_conv1 = weight_variable([12, 12, 1, 25])\n",
    "    b_conv1 = bias_variable([25])\n",
    "    h_conv1 = tf.nn.relu(conv2d_1(x_image, W_conv1, [1, 2, 2, 1], 'VALID') + b_conv1)\n",
    "\n",
    "  # Second convolutional layer -- maps 25 feature maps to 64.\n",
    "  with tf.name_scope('conv2'):\n",
    "    W_conv2 = weight_variable([5, 5, 25, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(conv2d_1(h_conv1, W_conv2, [1, 1, 2, 1], 'SAME') + b_conv2)\n",
    "\n",
    "  # First pooling layer.\n",
    "  with tf.name_scope('pool1'):\n",
    "    h_pool1 = max_pool_2x2_1(h_conv2)\n",
    "\n",
    "  # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n",
    "  # is down to 5x3x64 feature maps -- maps this to 1024 features.\n",
    "  with tf.name_scope('fc1'):\n",
    "    W_fc1 = weight_variable([5 * 3 * 64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "\n",
    "    h_pool1_flat = tf.reshape(h_pool1, [-1, 5 * 3 * 64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool1_flat, W_fc1) + b_fc1)\n",
    "\n",
    "  # Map the 1024 features to 10 classes, one for each digit\n",
    "  with tf.name_scope('fc2'):\n",
    "    W_fc2 = weight_variable([1024, 10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "\n",
    "    y_conv = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "  return y_conv, W_conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. write the code to train the model written in (a), train for 15 epochs with a  batch size of 128. \n",
    "\n",
    "Loss Function, Accuracy and Training Algorithm\n",
    "\n",
    "- You will use the cross entropy loss function. The loss function is called tf.nn.cross_entropy_with_logits in tensorflow\n",
    "- Accuray is simply defined as the fraction of data correctly classified\n",
    "- For training you should use the AdamOptimizer (read the documentation) and initially pick the learning rate to be 0.05 (if this learning rate does not work, pick different learning rate) with decay step of 0.95 every 2000 iterations as showen in the code below. You are encouraged, to experiment with other optimisation procedures and learning rates. [25 points]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll use the cross entropy loss function \n",
    "y_conv, W_conv1 = deepnn(x)\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=y))\n",
    "\n",
    "# And classification accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# And the Adam optimiser\n",
    "global_step = tf.Variable(0)\n",
    "learning_rate = tf.train.exponential_decay(0.05, global_step, 2000, 0.95)\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(cross_entropy, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready to go ...\n",
      "Epoch: 003/015 cost: 0.196267171\n",
      "Train Accuracy: 0.937\n",
      "Valid Accuracy: 0.899\n",
      "Epoch: 007/015 cost: 0.126739737\n",
      "Train Accuracy: 0.945\n",
      "Valid Accuracy: 0.907\n",
      "Epoch: 011/015 cost: 0.081585240\n",
      "Train Accuracy: 0.976\n",
      "Valid Accuracy: 0.909\n",
      "optimization finished \n",
      "\n",
      "The best valid Accuracy: 0.910\n",
      "The test Accuracy for best valid Accuracy model: 0.960\n"
     ]
    }
   ],
   "source": [
    "# Start a tf session and run the optimisation algorithm\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "model_path = os.path.expanduser(\"~/Desktop/best_model.ckpt\")\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Training params\n",
    "training_epochs = 15\n",
    "batch_size      = 128 # use mini-batch\n",
    "display_step    = 4 # display results every epoch\n",
    "valid_acc_best = 0.\n",
    "test_acc_betst =0.\n",
    "\n",
    "print (\"ready to go ...\")\n",
    "# optimization code, we used here stochastic gradient descent SGD.\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(train_dataset_flat.shape[0]/batch_size)\n",
    "    # iterate\n",
    "    for i in range(total_batch):\n",
    "        batch_xs = train_dataset_flat[batch_size*i:batch_size*(i+1)-1]\n",
    "        #print(batch_xs.shape)\n",
    "        batch_ys = train_labels_encoded[batch_size*i:batch_size*(i+1)-1]\n",
    "        #print(batch_ys.shape)\n",
    "        feeds = {x: batch_xs, y: batch_ys}\n",
    "        sess.run(optimizer, feed_dict=feeds)\n",
    "        avg_cost += sess.run(cross_entropy, feed_dict=feeds) # to calculate the cost\n",
    "    avg_cost = avg_cost / total_batch\n",
    "    # display\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        feeds = {x: batch_xs, y: batch_ys}\n",
    "        train_acc = sess.run(accuracy, feed_dict=feeds)    \n",
    "        feeds = {x: valid_dataset_flat, y: valid_labels_encoded}\n",
    "        valid_acc = sess.run(accuracy, feed_dict=feeds)\n",
    "        feeds = {x: test_dataset_flat, y: test_labels_encoded}\n",
    "        test_acc = sess.run(accuracy, feed_dict=feeds)\n",
    "        if valid_acc_best <= valid_acc:\n",
    "            valid_acc_best = valid_acc\n",
    "            test_acc_best = test_acc\n",
    "            save_path = saver.save(sess, model_path)\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print (\"Epoch: %03d/%03d cost: %.9f\" % (epoch, training_epochs, avg_cost))\n",
    "            print (\"Train Accuracy: %.3f\" % (train_acc))\n",
    "            print (\"Valid Accuracy: %.3f\" % (valid_acc))\n",
    "\n",
    "print (\"optimization finished \\n\")\n",
    "print (\"The best valid Accuracy: %.3f\" % (valid_acc_best))\n",
    "print (\"The test Accuracy for best valid Accuracy model: %.3f\" % (test_acc_best))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. [Extra credits] Find better convolutional neural network architecture that give better results (at least enhancment of 3.0%) than the one built in part b (prove experimentally). [15 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  \"\"\"max_pool_2x2 downsamples a feature map by 2X.\"\"\"\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "#define a new CNN\n",
    "def deepnn_better(x):\n",
    "  # Reshape to use within a convolutional neural net.\n",
    "  # Last dimension is for \"features\" - there is only one here, since images are\n",
    "  # grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n",
    "  with tf.name_scope('reshape'):\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "  # First convolutional layer - maps one grayscale image to 32 feature maps.\n",
    "  with tf.name_scope('conv1'):\n",
    "    W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "  # Pooling layer - downsamples by 2X.\n",
    "  with tf.name_scope('pool1'):\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "  # Second convolutional layer -- maps 32 feature maps to 64.\n",
    "  with tf.name_scope('conv2'):\n",
    "    W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "  # Second pooling layer.\n",
    "  with tf.name_scope('pool2'):\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "  # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n",
    "  # is down to 7x7x64 feature maps -- maps this to 1024 features.\n",
    "  with tf.name_scope('fc1'):\n",
    "    W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "  # Dropout - controls the complexity of the model, prevents co-adaptation of\n",
    "  # features.\n",
    "  with tf.name_scope('dropout'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "  # Map the 1024 features to 10 classes, one for each digit\n",
    "  with tf.name_scope('fc2'):\n",
    "    W_fc2 = weight_variable([1024, 10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "  return y_conv, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll use the cross entropy loss function \n",
    "y_conv_1, keep_prob = deepnn_better(x)\n",
    "cross_entropy_1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv_1, labels=y))\n",
    "\n",
    "# And classification accuracy\n",
    "correct_prediction_1 = tf.equal(tf.argmax(y_conv_1, 1), tf.argmax(y, 1))\n",
    "accuracy_1 = tf.reduce_mean(tf.cast(correct_prediction_1, tf.float32))\n",
    "\n",
    "# And the Adam optimiser\n",
    "optimizer = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy_1)\n",
    "# global_step = tf.Variable(0)\n",
    "# learning_rate = tf.train.exponential_decay(0.05, global_step, 2000, 0.95)\n",
    "# optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(cross_entropy_1, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready to go ...\n",
      "Epoch: 003/015 cost: 0.337665183\n",
      "Train Accuracy: 0.866\n",
      "Valid Accuracy: 0.900\n",
      "Epoch: 007/015 cost: 0.253496353\n",
      "Train Accuracy: 0.898\n",
      "Valid Accuracy: 0.918\n",
      "Epoch: 011/015 cost: 0.201691544\n",
      "Train Accuracy: 0.913\n",
      "Valid Accuracy: 0.922\n",
      "optimization finished \n",
      "\n",
      "The best valid Accuracy: 0.922\n",
      "The test Accuracy for best valid Accuracy model: 0.973\n"
     ]
    }
   ],
   "source": [
    "# Start a tf session and run the optimisation algorithm\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Training params\n",
    "training_epochs = 15\n",
    "batch_size      = 128 # use mini-batch\n",
    "display_step    = 4 # display results every epoch\n",
    "valid_acc_best = 0.\n",
    "test_acc_betst =0.\n",
    "\n",
    "print (\"ready to go ...\")\n",
    "# optimization code, we used here stochastic gradient descent SGD.\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(train_dataset_flat.shape[0]/batch_size)\n",
    "    # iterate\n",
    "    for i in range(total_batch):\n",
    "        batch_xs = train_dataset_flat[batch_size*i:batch_size*(i+1)-1]\n",
    "        #print(batch_xs.shape)\n",
    "        batch_ys = train_labels_encoded[batch_size*i:batch_size*(i+1)-1]\n",
    "        #print(batch_ys.shape)\n",
    "        feeds = {x: batch_xs, y: batch_ys, keep_prob: 0.5}\n",
    "        sess.run(optimizer, feed_dict=feeds)\n",
    "        avg_cost += sess.run(cross_entropy_1, feed_dict=feeds) # to calculate the cost\n",
    "    avg_cost = avg_cost / total_batch\n",
    "    # display\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        feeds = {x: batch_xs, y: batch_ys, keep_prob: 0.5}\n",
    "        train_acc = sess.run(accuracy_1, feed_dict=feeds)    \n",
    "        feeds = {x: valid_dataset_flat, y: valid_labels_encoded, keep_prob: 1}\n",
    "        valid_acc = sess.run(accuracy_1, feed_dict=feeds)\n",
    "        feeds = {x: test_dataset_flat, y: test_labels_encoded, keep_prob: 1}\n",
    "        test_acc = sess.run(accuracy_1, feed_dict=feeds)\n",
    "        if valid_acc_best <= valid_acc:\n",
    "            valid_acc_best = valid_acc\n",
    "            test_acc_best = test_acc\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print (\"Epoch: %03d/%03d cost: %.9f\" % (epoch, training_epochs, avg_cost))\n",
    "            print (\"Train Accuracy: %.3f\" % (train_acc))\n",
    "            print (\"Valid Accuracy: %.3f\" % (valid_acc))\n",
    "\n",
    "print (\"optimization finished \\n\")\n",
    "print (\"The best valid Accuracy: %.3f\" % (valid_acc_best))\n",
    "print (\"The test Accuracy for best valid Accuracy model: %.3f\" % (test_acc_best))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. [Extra credits] Visualize all the 32 filters in the first convolution layer. Each of shape 12x12x1, they might be viewed as greyscale images. [15 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/yujieli/Desktop/best_model.ckpt\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAADuCAYAAAC5+oUNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnWuMndV1/p+5esb2jI1dSFJqDISbyy1cjYnB3AkhBRNI\nA0makqaqGrVqpCpSL2o+tGpVtVJaVYrUD0GN1IZAabiYOwQMBuJwJ+YSxxAKDgUHbMDX8cwcz5z/\nB7QXv73HZzznlWde/tLz+7T8zjnH593vfvd519prPauj2WzKGGNMe3TW/QWMMeb/R7x4GmNMBbx4\nGmNMBbx4GmNMBbx4GmNMBbx4GmNMBbx4GmNMBbx4GmNMBbx4GmNMBbrbeXFfX19z7ty5kqStW7fG\n8YGBgbAbjUb2njlz5uz1s8bGxvb6Gh4fHx/P3tPV1RV2+h6bNm3S1q1bO6Z8EvuZOXPmNA844ABJ\n+fft6PjwK+3Zsyd7z6xZs/b6uqGhobD7+/vD3rFjR9idnfnvHf+9e/duSdLIyIgajUZtYzJv3rzm\nQQcdJEnauXNnHGc1W3kevLYck127doXdas7wvVI+3vyst956a0uz2Txw6meyfxkcHGx+7GMfk5TP\nFY5FOVeGh4fD5rm0en+aA+XrJWn79u0T/jY2Nqbx8fHa5kp/f39z3rx5E45zrvC6S/l90tfXF3Y5\np/Z2vLs7X/L4/vS3t99+W9u2bdvnmLS1eM6dO1eXXXaZJGnVqlVxfMWKFWFv2rQpe89pp50WNk+C\nF3Lp0qV7Pc4bR5I4yGeeeaYk6dprr23nFPY7BxxwgL75zW9Kyhe53t7esDdv3py95/DDDw+7p6cn\n7J/97GdhH3fccWE//PDDYc+ePTv7LF78X/ziFxM+pw4OOugg/eu//qskae3atXGcP6z8cZCkwcHB\nsDkmzzzzTNjbtm0Lm3OGP96S9M4774TNH6pvf/vbG6d+Fvufj33sY/qXf/kXSfkCwO//7rvvZu9Z\nv3592BwXLqqca3x9uVDcf//9E/72/vvvt3cS+5l58+bpq1/96oTjPD9ed0l6/vnnwz7mmGPC5pzi\njwuP/8Zv/Eb2WUcffXTY6Qf/T//0T6f03dtaPLu6umIBO+uss+L4K6+8EjZvAil/QuWJ8sL++te/\nDvuEE04I+5e//GX2WelpU5JeeOEFSfkvbR0MDQ3p6aefliR9/OMfj+Onnnpq2OWC9+yzz4b927/9\n22EvWrQo7Ndeey3sT37yk2GX58vPXrBggaSJN81M09XVFfOA15bf9c0338zek767JB111FFhcyH8\nzd/8zbAfffTRsPl0K0knnXRS2Mkr+CgwMjKijRs/WL/5ZMWFNP09cdFFF4Wdnlol6eabbw6b9wUX\n4oMPPjj7LI7xHXfcIWni0+lMw7nCceCDVvnwQa8jPTBI0pFHHhk258Txxx8fNn+ApPxeTNekXKxb\n4ZinMcZUwIunMcZUoC3/bnh4WD//+c8lSStXrozjr776atjlI/bbb78dNoPhBx74YdyergpjeOXj\ncxlPlep328fHxzUyMiIpD2w/9thjYdM1l6QlS5aETfeN7jbjWHRHnnjiieyz6KImd5cxoTrYuXNn\nxDp5TjyP8to++eSTYTPuzfcwppzGXJoYx+KcS/P1o8Du3bv14osvSsrda87/coN13bp1YR977LFh\ncw7xvmKY44033sg+i3HAr33ta5Kk6667rr2T2M9s375dDz74oCRp+fLlcZxud3n/cINww4YNYXNP\nhGEzzi268JJ06KGHhp3Cb+WmXSv85GmMMRXw4mmMMRVoy23v7+/XiSeeKCl/rKY7wJ1hKXe16JLz\n0ZuuCt250u04+eSTJ7yuzPGbabq7uzV//nxJuSvJsATTLiTpkEMOCZtpFE899VTYdOvodnBHXspT\nedI4lv/fTDM4OKgLL7xQUu523nPPPS3fw91VzgfuED/00ENhp8+XpP/93//NPouho9Klr5P58+dH\nqh/DOsxCYLhGyt3Sl156Kexzzz03bM4Vhjm4Cy9Ja9asCTuFmDhn62DWrFnhOjMDg5kCnP9SPl6L\nFy8Om5kV/CzeD5w3knT33XeHvWXLFkl2240xZlrx4mmMMRXw4mmMMRVoK+Y5a9asiGneeeedcfzS\nSy8Nu0wNYfoAY3qnnHJK2ClmKEk//elPw2YVk5THslI1T93VNLNnz450IaY9MJZS1uYyXYKll6zO\nWrhwYdispim5+uqrw07ljyydrYP33ntPN910k6Q8Hs74ZRm7ZSUaSxT5fqZlPffcc2GzlFXK4+yv\nv/56u19/2hgZGYn4LGObLNct49X8/twzYAnuYYcdFjZjpOW9kfYrpA+reereM+jt7Y09AKbtMa7J\nvQ4pr0Ti3gLHh2mNTKvkGiTlY58qJac6Jn7yNMaYCnjxNMaYCrTl8+7ZsyeqN+hCpS1+aaIwCP/G\n1Amm2FAhhypMrBSRcrf4owJDGXfddVccZ9pEKa1Hl4Ln/vLLL4dN9+LKK68Mm+MmSbfffnvYyRWu\nu+qqr68vRGAoPEGFI4rJSLkgDN17Qpc2qWpJ0r333pu9rkyXSzzyyCP7+urTyvbt2+O78l6g60mV\nH0l66623wm4lnMF0I7rmpcgI780U2iiFMmYaCoMwFMExoSCMlK8prG5kKITzjimPk4V40v8z1THx\nk6cxxlTAi6cxxlSgbT3P5FLRxfzEJz7R8j0UeeDOOyshuEuWBEmlieIR1MtMO9it1KNnij179ui9\n996TlFc70HUs9SYpCszdVO7Ks8KCO4/lLjV3BpPeY92uWF9fX7ifrA6hwAOzCaTcfaJLS1efu7Gs\nIFm2bFn2WXVrVLaiu7s75jddSVYIlSLRrEaju8oqLH4Wx6XcbWe11//93/9Jyse9DsbGxuI7MLuG\noacy7MW5Q1f/vPPOC5tVZ3w/xaKlPJSR7uMyO6YVfvI0xpgKePE0xpgKtOW2M8mXO590FZj8LuW7\nzuVOcYJuCz+LbQek3IVLn1u3i9poNOI78/v98Ic/DLtscMUd9qm4CHTRSiEHZiScc845E75HHXR2\ndob7yawMulIMwUi5i8kxYcI8rzXHgSIbUp74XCZY18no6Gi4y0cccUQcZ5J7OR/otjP8w/YRzDxg\n2KvU1i13raX6+111dHRE6I3zg4IuTPyX8vPg/GKhBUNdDO2V40sRkHSfOkneGGOmES+exhhTgbbc\n9tHR0ejqSNeQiadl7Si7J9JV4Y4Z25/SNWNSsJS79OnRutyJm2nGx8c1OjoqSfqt3/qtOH722WeH\nzTpsKU98ptvE3WiGO2688caw6eKV/2cKFZTta2ea3bt3RysQZmLQ1aaGgZSHH+iW8ZrTrTv99NPD\nLneomeHB+Vc3TAin+0ibmp1SPo+oAfHlL3857NTaQ8rHq2zJzGyFtDNNjc866OjoiOvHFiK8pmXY\ni0UUXIeYAcTX0IUv759070ofrimlpmor/ORpjDEV8OJpjDEVaDtJPrmTfCymG0rXQsrdcO6C0gWj\n28HkcLYakHIXPT2K150k39fXFwn/zECgq1AWETBMwaR3ugt8DSndrBUrVoSddijrlukbHx/Xrl27\nJEmrV6+O43SRGJaQ8nHgrjJtFheQMruArmurOvk66O3tjfnN7o4MWZT3D7ULPvvZz4Zd1q0nWHRR\njjHnZwp71F1Q0NPTE1k1dNWZTVCG5hiWOv/888NmKCfNPykPF5UhAIYKki4A793J8JOnMcZUwIun\nMcZUwIunMcZUoO1UpVQhwbQHxtjKGCTTTigSwfQmihOwUP/pp5/OPuuKK64IO8VzpppWMF0MDw9H\nigRjm4y1lZVVF110UdhMQ2J1CKscJqsaYew4pWHUnb7VaDQijse5wblQpm8l/U9JOuqoo8Jm+lZq\n7SFNXplFYZFSE7ZOZs2aFfFxxvYZdyzjbbxPXnjhhX3+H4wdM2VLymOeSQez7tbDY2Nj8T1bpSyW\n6UU333xz2IzrMhbK9zCuWe4H8D5162FjjJkBvHgaY0wF2nLbu7u7ozKIent0K0udRroO/BtdXLbe\noAtDzU8pb92QXBCGD+qgq6sr0kvWrl0bx+luXnPNNdl76Hoz1YLVWXS96c4kzcEE3dc0pnWPyZ49\ne+Icef1pl60y+G8KPLBlCecMx6fsOMnKtFJLtW7S9br11lvjGMelFMNhBRnTbBjS4rgw7Y9zUJpY\nvSTl6WN1sGfPnnCXOc8ZTii7wTI1jS42BWJadWAt743LLrss7KRB61QlY4yZRrx4GmNMBTracfE6\nOjo2S9p7aUN9LG42mwfu+2XTg8dkIh/RMZE8LnvDYzKRKY1JW4unMcaYD7DbbowxFfDiaYwxFfDi\naYwxFfDiaYwxFfDiaYwxFfDiaYwxFWirPHPu3LnNVGJJNSMqlZQqRyyfaqX2wxKxVmrh5etSitX2\n7du1e/fu2uSwBwYGYkxaNfUq1bo5XiwlZPkqy9OYTlaW07Gneyrzq3tMZs+e3Uwlq63OtZwLVNFh\nSSVVb1r15S77bHOM+Lm//vWvt9SZ09jf399MJZa8vpwf5fzv6+sLmyXRfA8V2Dnv+F4pH/M0Lu+9\n95527txZ21zp7e1tlg38pHzdmKxckqWavO6txqEs1+Vnp8969913tWPHjn2OSVuL58KFC/WXf/mX\nkqTFixfHccpC8biU12Lzi1O6ji0F2O2wzEF94403wk6Dc/3117dzCvudhQsX6m/+5m8k5Tc32wCU\niye1ANg98ytf+UrYlJrjDfWrX/0q+yy2c3jwwQclSTfccEN7J7GfmT9/vr7+9a9Lys+VrRXKzqis\nz/7JT34S9pe+9KWwWeN88sknh80fEEkhm1h+7j/+4z/Wmow9b968uMa8vlwA+N0l6dhjjw2bdeuc\nU5/61KfC3rZtW9jsVivlGgBJHvCf//mf2zuJ/Ux/f78+/elPS8oXPK4bp5xySvYerguUnuO9wXuR\ncodl19aTTjop7DRef/d3fzel797W4tnX1xdfhH1iKDjw+OOPZ+/hgnn55ZeHzV48fOLiQlE+xVIo\n5Oc//3k7X33a6O3tDQ1Ffl9eyLJvCn9V2dfp4YcfDpuLDn8dKY5Skn7EyiexmaazszM0NblI8Oal\nhquUT3aKpVALlce5eJQasieccELY1HKsG2q/cpGjgAfb50r5gsJ5QM1SLpK//OUvwy61K/lgkoRb\n+DRbB7Nnz44fQgp7pNbIUn7dpVyAh/3T+ISZ+hFJ+fpU/tCeeeaZYacfZz74TIZjnsYYUwEvnsYY\nU4G23PZGoxHuOh+F2TahdCuXLVsW9t133x02YxWMYdC1K9vvUvdwyZIlkiYGxWeaXbt2hQ4ntRjp\nbpVBarqyfN0hhxwSNmMzZZyG0O1KrYcn23SbCXbv3q1169ZJyucGXdVSq5WhjfXr14fN8AWvPz9r\n+fLl2WcxBEBdx7ph62HOeZ7LxRdfnL2H58I2JhdccEHYDIHwninddr4/bVjVHeIZHh6OEBzj2wwx\nPPDAA9l7GC/n2PFeeu2118Lmngy1g6X8OqT48r333jul7+4nT2OMqYAXT2OMqUBbbvu2bdt0//33\nS8rda+58stuf1Lo1xeuvvx52K1n9MqeRLQlWr14taeKu7UwzNjYWrtWhhx4ax5lqUXb/Y+4hz52d\nD3muTLcpc97oyqb333fffW2dw3SQ3EFew7POOivscoecoYwUkpFa73xyR51zTMrT5VIajCR973vf\nm9J3ny7GxsbCzRwYGIjjvJfK9iTsOPv5z38+bLqb7CpK1/zAA/OUVoZz0rypW5Kyp6cnvjNT2S69\n9NKwyy66zKDgvcS0Pa4Lk+22M/0xrU97yzvdG37yNMaYCnjxNMaYCnjxNMaYCrQV8xwYGIi41fPP\nPx/HmaJTptW0aovKtAKmNzFeVcZjmMKS0gpYlVAHrLpKLVSl1qk3Ul5RxZSMt99+O2zGMhkXLStm\nWPud4ot1x7H6+/sjJslKK44PY35SHqNiDJA2Y8LPPfdc2GU7ZsbIli5d2vb3ny56enriHFrNgbvu\nuit7D69v0guQ8nQaxsdZMVNWtjH+nO45xgzroNlsRix25cqVcZyxbqYjSXl64ooVK8LmmsLKNN4P\nvJekPI0wzamydLgVfvI0xpgKePE0xpgKtOW2b9++PVKVzjjjjDjO9KKyuoWuGqsl6GoxbYXCBlSL\nkXK3I7m4/L/rIqXlsLqD7kCZvrVp06aw6a5yHOhqMKxBd0/K0zCSq9xK+m+m6OzsjHQPhmFauVLp\nPQmmfH384x8Pu1XaG0NDUj7P6OrWzejoaKRkpftIks4999ywy8orVgDx3qL6El1X3m9lhR7fn9KC\npuqiThcMBfL7vfrqqy3fw9AEx5Eyf63EUijEIymqA6WJ6mf7wk+exhhTAS+exhhTgbbcdpJEKKRc\nm5M775L0ox/9KGzuoHFnjOIPrXYhpb1X7bT7qL2/aTQaUaVAN4muernbzt1R7hzyXBii4I4rRQ6k\n3N195JFHJNVfddVoNEJnkbqTDCeUlTSs6mCF1PHHHx829VK521xW0vCzfvzjH7f9/aeLRqMR2RIc\nl1ZZGlIewkhiK1J+jvwsal/ecccd2WfxPamqp+7d9u7u7gjNMATH++LKK6/M3vP3f//3YVPomDA0\nRuEYCoZI+b24YcMGSVMPe/nJ0xhjKuDF0xhjKtBuA7jY2XviiSfiOHcIS5GGwcHBvdp8LD/66KPD\nphtKsQgpd+GSjmXdO8sdHR1xLnS7JxMjYOI4z4k7hxwTtue48MILs89K7RSkD7UKWYxQBywcYLI/\nd74Z9pHyXV+6YtwpZaiHO67J3UowPPRRasMxNDQUPasoksKdcwqZSHmCOMePrijvE54vXVcpF+NJ\n7jrHtA62bNkSgi3sVcTvdeutt2bvYcjnhRdeCJttfjgnmPnCLA1JuvHGG8NOYaFSkKgVfvI0xpgK\nePE0xpgKtOW2j4yMRPIq3W66jmWbUL6ObQHYOvaee+4J+5prrgmbtcxS3rEztXeouw1HR0dHJDJT\na5PtH8rd4FaJ3+wYmHbOpXwcSreD7k3qvMhk4TrYuXNnnBfPleGLMkmebhavKRPI6ZLS5m61lLvt\nzHqg21oHg4ODOueccyTl48IQRqnXytp+hoI4p9iemPPuoYceyj6L7m7Kiilbdcw0/f39Ebpiy2mG\nKMp25pwfHDuGKfh+FtuwgELKXforrrhC0sRxa4WfPI0xpgJePI0xpgJtPbP39/eH2/joo4/GcSYv\nswOklLuVlCFj8vJnPvOZsLlzWD5iM8l3bzvcddDV1RWJ6+xkyXMtz4PnyCRd7sIzg4EuSLl7zJ3p\ntPP4UZCkS+4y20KkLolSntgt5dkFdGmZGM4kahZTlAn3TDTnbn3dsA0Hd9i5S7xx48bsPQx10FVn\nPTs1A37xi1+EXcqvMeE+1XRzztbB6OhojAWvO9eNMizD0BWzNDh2nB8ca2a0SHkGUCroKTUBWuEn\nT2OMqYAXT2OMqYAXT2OMqUBbMc/x8fGIMTDFgXGZUs+T8QnG+igGwlgm4xNlFQpTUFLKVN1pOWw9\nzPgJBQgYE5byCgYKM1AIJaViSbkgSinkcOedd4adqrPqjnk2Go24juU1TDC+JeXXnTBNhXHRgw8+\nOGym8Eh5C9uLL744bFZq1UFfX1+cA+ctY5ElfB1T1hjL5T4D48KMF0v5/ZcqtFihUwd9fX2hvcnY\nLeOcpRgOUyP5ty9+8Yth87x4v5XzkRWNaR2aqliKnzyNMaYCXjyNMaYCbbntw8PD4S4wlYBdH8u2\nB9RmZPoNBSP4iE0tSrrzknTSSSeFnbp31p2qNHfu3BBzoIvFcSjFF5imwzDHqaeeGjbdL4YDStEC\nurKPP/74hPfWQaPRiBAE3a+rrroq7DKUwetIMQyeH9O/GCpasmRJ9lmscrv55pvb/v7TxcDAQFRM\nsRKI90WpN8nwFqul/u3f/i3spCcr5fNu2bJl2WftLbxWd4hn69at0TGU58EKxLL1DFP6mI7FCjKm\nB7Ja7/d///ezz2KFXxJtcfdMY4yZRrx4GmNMBTraeWzv6OjYLGnjPl84syxuNpsH7vtl04PHZCIf\n0TGRPC57w2MykSmNSVuLpzHGmA+w226MMRXw4mmMMRXw4mmMMRXw4mmMMRXw4mmMMRXw4mmMMRVo\nqzxzzpw5zaRiwtJJNlEqSyqpCEQlprI8L8Hyw8mau6Wma++++6527txZW43m3LlzmwsWLJCUf3ee\na6k0RdUW2mz2lc5PynvTlw3CSFJQ37hxo7Zs2VLbmAwMDDRTSR3Pg2WqLMMtX8dSTc4nlqmyhK5M\nt6MSOEtVd+zYsaXOnMa5c+fGuFDBnde3LK1lSSrhOXO8qDzGMZXyey7NyR07dmj37t21zpWkkN9q\nzpdq963mSqv7h2NVNrzjvZnKVzdv3qwdO3bsc0zaWjwPOOAAffOb35SUd+xjfSjbQkjSTTfdFDZr\nVCk1xgFgfWr5WZSPSif6T//0T+2cwn5nwYIF+ta3viUpr7llTTK7fkp5a4rUxVDKWyiwRpkLBWt+\npXxMkvxakhuri4ULF+rb3/62pPw8qGfANizSB+OY4E2eOitKeXfR5557Luxywfnv//7vsItukrUm\nYy9cuFB//dd/Lam1ngO/ryRdcMEFYXNBoY4Cf2CSvoE0ceHlPZf0KDhWdXDggQfqH/7hHyTl7VP4\nI8D2LVJ+XlwMX3nllbBZx88f7bKTLe/NpAWQrtG+aGvxHBoa0pNPPilJWr9+fRynZmeprUgxAxb+\nr1q1Kuwrr7wybA4AnzSkvLXoJz7xCUn5r00djI+Pxy8jJye1E1O72QSfjLj4ES4gvKF+8IMfZK/7\nsz/7s7DTmJY9k2aaoaGhEFngrz5/UKhXKuU6ixw72nxKZ+8n6qBKudBG3SIphDqn9CD4/dO8TvAH\nh6/jWPIeYG+jshcPF9/0wFK3sM74+HgslPxBeeutt8K+5JJLsvdQp5QiO600dHmPPf3009lnUYwn\nXZvSU2yFY57GGFMBL57GGFOBttz27u7uaJ9AF2qy1sNPPPFE2IyTMqCbXDxJWrlyZdgbNmzIPost\nZpMLU2plzjTNZjPiT3TFGFvhppmUB8DpYh922GFhp9awUj5uZSiDWoUpZlMGxWea7u7uaI/AzcTT\nTz897Ouvvz57D8+RG4WcP2yjwVDRX/zFX2SfRY1HumW33XbblM9hOmg0GhF64nX/1Kc+FXZ57dhO\nl+EIuuRs48Ex4nyScm3V5CJPVbtyutixY4dWr14tKV9TrrjiirDL0MvZZ58dNucH7w3OIcaUk/Zu\ngvsq6b4sN9pa4SdPY4ypgBdPY4ypQFv+XWdnZzwO00Xl4zZ31KXcpaAbTneOrgm7KpZd7F588cWw\nk9tR5gvONB0dHeEmcueS7hDblJSvY6oSMxgYCuFuKnefpXx3NrU8YRuLOujp6Yl0LHZ5ZKYAu1pK\n+bVlxsby5cvDpqvLzIvUxiHBNhyt8onroLOzM1JomF7E+cH7onwd5xR3mRniWbp0adjsRinlGQqp\nvQnT4+pgdHQ00oVSixIpD0swXCHlrjo7aTJkyNAYMznKbqEMCaTMBrfhMMaYacSLpzHGVMCLpzHG\nVKDtmGeKtbAE8Cc/+UnYZQySpYiM7bAah6VU/KyypSxL/S677DJJ9ZeXNRqNqIZgfJexmLJMjv9O\naRpSXnLHtrEvv/xy2AcffHD2WUxtSWNdd5xvaGhIzz77rKQ8DYfjU6ZvMRbF68wyTJb3rlmzJmyO\nm5THzS+66KJ2v/600Ww2o3qFMT2mFC1atCh7D9OLOH48R8Z/GRN/7LHHss/6/Oc/H3aqUPoopLWl\n6/rOO+/E8RNPPDFsxjilPA2Q8V7OKe6XsH11eW8wxpxinazEmgw/eRpjTAW8eBpjTAXaemafPXt2\nPE7/x3/8RxynC18KXdA9Oe2008K+/fbbw2a6TqpMkfJ0DCl34VLaE9VX6qDZbEaVE1WVWAFTugFM\nVWLIgq9j+KOVVJeUu/Hpe9Qt9sCqqyS2UFLKDaYwjJSHPOiGUnGLc6MUQuFnt5J0q4Oenp5wtyn2\nwmtNZSFJOuOMM8JmqtGPfvSjsBnWYVob77eSNFfq7p7b29sb4YiHHnoojjOVrUy947VnBRnTJJmC\nxSrE8nw5v1LIZKr3j588jTGmAl48jTGmAm3reaaqBRbh050uKyT4WE2X4k/+5E/CvvPOO8Om20KX\nVpK+/OUvh512zeoWQ+7s7IwqEGYTcDeUQhVSrhfIMAfFfrnzSFe91BrkLnXaQa1bw7LRaITrzWtY\nKoITivhyzlCjkcLIdL9KARnOM+rDfhRI15tCKJwDpZ4nw17ceadbyjAHP6vcSWf4J1XdlFV8M83o\n6GiEu1gdde+994bNjB0pv74pq0PKw4cMhVD7tJwrzAZJVZNTzVbxk6cxxlTAi6cxxlSgbbc9uYl8\nlKYLdc0112TvoavAlgJMguXuFhOey10vtiFIruxUJfOni0ajEd+Lrjo1SsvEdragoGgBXQ1mEfAc\ny9YKdGWTi1u3297R0RHuIOcGWyuwZ5GUh2uYcXHEEUeEzTGhO84+P+n/T5TJ+HUyMjISu7vMLmDY\nq9SnpRvOEAbDY3Rj2dKG4yjl8zCJ0Ew1IXy66Ovri/uB15rnV/YwOuuss8KmoAzDf2xHwrnFjAUp\nzwZJa89UMxD85GmMMRXw4mmMMRVoy23v7e0Nd507eWyP8dOf/jR7zwknnBA2XS26lqzZ5e5xuTvL\nxOIkn193bW5vb28k19KF4A5fmTXw6quvhs0Eb+60MizCcEfZ5oR1vsm9r7u2ffbs2TrppJMk5d0K\nqWeQ/p5gAjyTnZnwzgyEyy+/POwym4G70qXLVyeDg4ORUfH888/HcbaCYKGFlM8d7rBz3rfSPC3D\nXhzjFE6ru43N6OhoXG/Oed4LzCaQcp1Stg6mS85QVytNDSnPNkidfqmhOhl+8jTGmAp48TTGmAq0\n5fPOmjUrdsb4uEy7dKO5q0oXk/W4N9xwQ9hf//rXwy4T7rkLlh7F695Z7uzsjORaPu7TbS9321sV\nBXDXtay67yaNAAAYnElEQVT9TnAXvyRlMLR670yxc+fOkBakK8UwTOmKtcouoAwdsy3Y3oNhDSnv\n0smd6LppNBoRUmAXR8owUl5OyufUunXrwqYeBGXsmF3A+03Kxzjdp3XrIDBJ/pxzzonjzMxguE7K\n1wGGDFnzzvG55557wi51EBgWWbVq1V5f0wo/eRpjTAW8eBpjTAW8eBpjTAXainmOjIxEms3hhx8e\nxxnXLCXzGcNplZ7E44wBUixCymNmSS+z7lQltlZg/IXpFGX6CdMleE6M9TGFhDE9thSQcj3ClJZT\nd1UNU9oYp2Q1VSlIwfnEVtQ8P44vq2fYtkPKW5t8lFKVent7I/49OjoaxxmjLtvYMD2J9wljwWyV\nS/1PpnZJebvwRN0xT7ZjZpyT6Wqs3JNajwOrhThuZeyXcEyS2FGpSdzyu0/pVcYYYzK8eBpjTAXa\n9nnTIzPdIT76UlNPyjv40RVlhQndNOr4lS4qXdwkkFF3hcTY2FikNrCSgelJ7PAnSUceeWTYTKWh\ntuDatWvDpotbVj8wBJDaFdQdyuju7o5ULX4/uqp00aQ8dYlVadSNZeUUQxl08aQ8bPHSSy+1/f2n\ni4GBgUjHYRUUr1eZqkShHLqotJkqyNAPq5ik/J5LY1S3sE5HR0dcV64jFM8p3Whee7ZZYVocq5UY\nIizbwvDeSt16XWFkjDHTiBdPY4ypQEc73fM6Ojo2S9q4zxfOLIubzWbr7bRpxmMykY/omEgel73h\nMZnIlMakrcXTGGPMB9htN8aYCnjxNMaYCnjxNMaYCnjxNMaYCnjxNMaYCnjxNMaYCrRVxzc4ONhM\nZXdUcmEJWVnuxfIpKrhQrZllWewjXaZR8f9JJV27du3SyMhIbdIwPT09zaSKwx71PO9yTFgSxvPl\n+fGzWC7G41Lexz197vbt27V79+7axmTOnDnNpHRUft9EeZwq5yzHY3M3lufyNeU8oeoO379p06Yt\ndeY0zp07t5kUyDgnOB84DlKudM7z5DlyLFjqWfZkpxJV+tubb76p999/v7a50tfX10z3Cuc/z7ts\nBMn7gWPSaqxY1luOL8c+zZVNmzZp69at+xyTthbPgw46SN/5znck5S0j2A2S9d1S3qCeJ3HHHXeE\nzVpmLsqshZak1157LexUx/rggw+2cwr7nb6+Pp188smS8snJlgKsPZbyzoA8X8qvUZrv2GOPDZuL\nsiTddtttEz73+uuvb+sc9jfz58/XH//xH0va+w1bHpfyrqtsp0AZOnZWpYxbOU9YN84a6b/927+t\nNRl7wYIF+ta3viUpl+pjHXYp6bh06dKweZ6s3+d9xfYTpbzd7/7u74ad5t1VV13V3knsZwYGBnTF\nFVdIyuc/W6mUNfrUgOCCy7Hijwtr3kuZS479YYcdJkn6gz/4gyl997YWz61bt+qWW26RlAuAUK+S\nggVSLvjAp6QkYiHlJ7phw4awly1bln0WRRPSDTZV7b3pYnBwUBdccIGk/BeSY8KWr1IubHD88ceH\nTd1CPo0/++yzYfNiS/lESotTqZU50/T19YXeKq8n+w5xokv5Isfx4YLJBYev5/8h5edfCkHUycjI\nSIwB2w2vX78+7HQDJ/jjyh8cLho8f2pX8v+Q8h+vRx55RNLEBXamGR8fj15lZ599dhznfc2HKyn/\nceW9xXuODxlcMMueVtR+TWI1pWhNKxzzNMaYCnjxNMaYCrTltnd0dERQle1S6U6l+F+CsRkGdBnY\nZozqkEMOCbtsOUptxqT1WXcbgUajEeEEuoh0wUtdUn5nup9sV0x3leNWuuRsz5zGtNwomGn27NkT\n15QxcH6v++67L3sPQzR02U477bSwGd+ma89NISmfg2UMsU56e3tD35QbptSkZasSKXch6d5/5jOf\n2ev76caW7Sfooib3tW5ti8HBQV1yySWSpFtvvTWOM5THNs1S7nqneKmUzwmG0Lg5WbrkvH+S21/G\n0FvhJ09jjKmAF09jjKlAW277rFmzInWEqUqf/exnwy7TClrtsHOXmJ3uzj///LDLXdTPfe5zYadH\n9LpdVLpiTKvho3+ZXsRwBNuZtHI1+H6mc0h732EsdxRnmvHx8cinY/iCLvxkObzcXeX50X3juHEu\nSXnGB/92++23T/0kpoGurq7o9Mn0NY4RQzdSHvaiq8/WNXT1+RqmEJbvOeWUUyTVn60yMjISc5oh\nGs6VMizDFjccH4b/GOriODDFTcrXsTS/GIacDD95GmNMBbx4GmNMBbx4GmNMBdruUZtiVUw1SnEc\naWK8jXGMI444ImymCLAlKuOiZf0z41epQqLumCfTcsrUqsSpp56a/ZtxLdbashqL48jyuzKNghUo\nKfZVd/pWb29vpJz9+Mc/juO8/mU8jlUjbD/N1CxWzDAdqUzfmqyipE7YZpdxTsbqWE0m5a2XGVNn\nBRo/i9e+bGOcqr6kD++tuufKrFmzoqqK9z5Lksv0IqYE8m+szvrZz34W9uGHHx4255aUz6kUD51q\nHNhPnsYYUwEvnsYYU4G23PY9e/aEa3rttdfG8SeffDLsr371q9l7brzxxrBZPfTiiy+GzXQmuuZ0\n5aQPVGkSSUSA/3cd9PT0hJoRlZAo+MHKKCl3tRm+YIoEq0aowsR0EykPAaRUixTSqIvh4eFIP6Oi\nDd3TUgDjmWeeCZthIKaWsOqKY1267RQgufLKK8P+3ve+N/WTmCbSdeV35Bwo5zxZsmRJ2LxnmJ7E\nVLZSpIc89thjE/7vOuju7o77miEEfi+mpUnSo48+GjZV25h2xJDfmjVrwmZFkpTPT75uKvjJ0xhj\nKuDF0xhjKtCW297Z2RmZ+9QB5C7gK6+8kr0naV1K0jvvvLPX49wR5Y4bdwelXAw5uXN177Z3d3eH\ne0R3/I033gibIgdSvqO6aNGisBmWYAUOP/e8887LPovVWUmApKyimGk6OzvD/WR4htef5yrlIr58\nHUMQrDBilsKJJ56YfVaq+Co/t25GRkZiDvP8+R3pgkt5pkUr4W/uqv/O7/xO2MzekHJXeF9K/zPF\n0NBQVCVS2INhGSrHS7mo81NPPRU2x5RhIGbzlOPLMNgf/uEfSpIefvjhKX13P3kaY0wFvHgaY0wF\n2nLbe3p6IiGXBfl0MctmZ3QrmUTeqocLXYuyjQAf0dPjNgUl6mDnzp2hbcqd0lZtAKR8J/3QQw8N\nm24LE5/p+tIllXI3LYVS6k58lj4Mp1Dkg+PDeSHlQhC8puwFRVed41uGRag1e+GFF7b71aeN8fHx\nCMEwFMOxKAVTKHBBF5v3DHeMqZN6xhlnZJ+1bt26sNMYlW7sTMPWJEceeWQcZ/I7z0/K9XGph/vA\nAw+EvWLFirAZGivHl6HBNCZTzUDwk6cxxlTAi6cxxlSgLbd9aGgodorpdqxcubLle+iK0gXhrjrd\nNLowTz/9dPZZdOGS28bdyDpgaxLW47KVLvVOpVy3k+4qa785vt/4xjfCLl0KJgyna8Nk9DoYHR0N\nHU7uhDNcUbaIYM0+9Ro5DgwVsQiByfNS3mXyo7TbPjQ0FK4zE9hZFMAxkvJxott93HHH7fU4XXvu\nOEt5nXzSQag7SX54eFgvv/yypLyYhJkVzLKRcveemQY8d54r77dyfNeuXRt20jidagaCnzyNMaYC\nXjyNMaYCbbnt3BnjLvEtt9wS9he+8IXsPXTbuVvKHTTultId446ilCd/p5r2uiXHent743syzEDJ\nvlJGjjW4dNUZgqDkGJOjy51ltj1JY1G6JjPN2NhYhGWuv/76OM5k5bKGm+4167bp0rLD5nXXXRd2\nWa/MWneOb900m82YC8wWoHZBmYXAUAXDVpwHvE+YxVJmXbBwIxUilNkxM01HR0e4ydS+oNtd3uPf\n//73w77ooovCpvQj76VWmhFSHj5JYQO6+ZPhJ09jjKmAF09jjKmAF09jjKlA2204Eq+//nrYjM+V\nWpIs6me1CeMxq1evDpsZ/2UaEmOFX/va1yRNTGeaaUZHR6Magmk5TK8oWw/zHKlfSu1TxkmZylVq\nVzKVJ8UK645j9fT0RLoR9RZZNcK0LikXmqHAA6urGBe9+uqrwy5bVFMrlDHDupkzZ45OP/10Sfk1\nYtpemVrFsWAbZsYEKaTCiiHGmKU83pcqbdiuog4ajUak66WUJSlP9aNISAn1S5cvXx4245ZpzKVc\nR1XK4+vp/3GqkjHGTCNePI0xpgJtue0dHR2RLkQXgtv9ZXe6P/qjPwqb1QCpKkeS/vzP/zxspkCV\nbjtDBSn9YKqd7qaL7u7uqNZgWIFdQyeDbhNbaqxatSpsViixc6CUu/EpPFD3mLDC6L/+67/iOAUd\nKN4h5ePFkA5DFv/zP/8T9uc+97mwy3YTdGMpsFI34+PjEZ6gq85UmlK4ghVGdMn5urJaK8E0Nil3\nR1Nri7pT/RYsWBDpjQ899FAcZ/oWK8akfF3gPddK5IRjXZ4v75X0OqZXToafPI0xpgJePI0xpgId\npZsw6Ys7OjZL2rjPF84si5vN5t79lhnAYzKRj+iYSB6XveExmciUxqStxdMYY8wH2G03xpgKePE0\nxpgKePE0xpgKePE0xpgKePE0xpgKePE0xpgKtFWe2d/f30wlgFRoYYlgqV7NMkymRbEsiqrYVBMq\n06ioJJ/UhHbu3Knh4eHaGpXPnj27mdRYWvVLL9WrqajDsWuVNjbZazheaRyHhoY0Ojpa25hwnqT+\n7VJ+/SZrPEYVfo4Vz52qQlT1knKlKZZnPvPMM1vqzGns6elppjFgmSDPt5xDHDOWV/L9PM77rSxv\n5t/SvHnvvfe0c+fO2uZKd3d3M81vznOqsbFEV8rnAc+dc43jyMaTpSoZ/506XWzdulW7du3a55i0\ntXgODAzoqquuiv8gwW6H5ZejPBhPmheW3R5Zu122r6CU21NPPSVJuuuuu9o5hf3OvHnzdO2110rK\nJycvXllzS2k2SpCVLTYSbLNQvmbjxg/zi5P82po1a6b69acFzhMuZEuWLAmbkmNSPjcow8buojz3\n8847L2y2X5A+nBuS9Fd/9Vdhd3R01JqM3dfXF9+VUnvUNCgXPLYR4bik2vTy+OLFi8NmWwsp7zqZ\ndCK+853vtHUO+5uenp7Qs2BrHnYHfeWVV7L38GGEcnX8QeHDGXUTytY+XLsef/xxSdK///u/T+m7\nt7V4jo2NRdE8f/lZeF/2jHn11VfDTgIaUv4rwScH3iD81ZWkxx57LOwkhlC3CMaePXv07rvvSsov\nOMUp+AMi5T8w/FXkUxLHlH1tKI4i5Vqh6W/lj85M093dHT8KTzzxRBzn9S+1Ju+5556weUMcc8wx\nYfMpg2PIeSHlGp6cf3XT398fPyBcMCmIccEFF2Tv4X3CVsKcUxQW4bhu3rw5+yw+xafeUK28pZli\ncHBQF154oaR83vKannHGGdl7+KNA0RD2+uJCynEoBXv4/iTaMtUeYI55GmNMBbx4GmNMBdrdMIr2\nCXwsZoyJgV5JWrRoUdjJvZXyR2/GbOhyldqVbFeb3LEymDzTdHV1hZv55ptvxnHG90q3krEnhizY\nUoTu5ic/+cmwqWMpSS+88ELYyS0rXfuZZs+ePRHHZhyLIZayPQbDD7zOnD/UbuQYMq4q5a7++vXr\n2/7+00VPT0+EZji3Gb8tW0AwjEWXnCGtpUuXhk0XlSEhKR/XtDnCsEAd7N69O+K/PCe2aWEITMrb\nkzP2y3O55pprwqZmboprJtgGJ4UyuIE3GX7yNMaYCnjxNMaYCrTltjebzXANmX5D14xdEKU8n4/S\n+txR5Xu4i1p2EqQ8fkoLqnu3fdeuXeEKrFixIo6zq1/ZKfLhhx8Om+lNdE/omrz00kthl7vHdOmT\ni1p3a4X+/v74/kzLWrduXdinnHJK9h667XTP6bZyt56uenJBE+yQOFk+6UyzY8eO6C6bXEQp33ln\n2p6U7zSzIytDQQx7cfeYbW+k3B1N3SWnurM8XcyZMyfOkR0v+V3L63vzzTeHfemll4bNnXR+Fted\ncj5wTA8//HBJ+T05GX7yNMaYCnjxNMaYCnjxNMaYCrTdejiVj23YsCGOs9qojLe1imMwDsiSKcYn\nypQBxidSGkaZejDTjI+Pxzmy0oOpSmX6CVvl8m+sGmFZItMpylJPxoXT/1mmAc00O3bs0AMPPCAp\njzExJsyxkvJ4NiutGH9iKR1TfRg/lKSVK1eG3aotbx309vbG/gDHhelq5bVj2hlT2XiOjG0yfnry\nySdnn8U20ClOWnd8fGxsTO+//76k/F6gXcaBf+/3fi9snu8NN9wQ9he/+MWwuVaVezLLly8PO8Xn\np1qh5ydPY4ypgBdPY4ypQFtu+9DQkJ599llJuUvw8ssvh01VF0nxSC7lKRVMMaLbzyqDslqJf7vp\nppsk1V9hNH/+fF1++eWS8hQZptKUAg2t3II33nhjr++h61q67azUSulB6RrVRXd3d7hTPCeGIuhu\nSdIXvvCFsJmuxqotuucc61KJ6NFHHw27FCCpk66urpjrdMcZrinvH1YF8T5h2IP3Fcf1lltuaflZ\nKXQ01Wqa6YJpbTwnznOqqUn52HFOseqM6YCEaZVSHgpM4ZOpiqX4ydMYYyrgxdMYYyrQtjDICSec\nICnfEeVuWOmScle0VRUJKwNYecTdVSl/rE+ubN16hNu3b9fq1asl5bvJ3AGl6yrl4QiKANOFYuUH\n319WK9Fl/fSnPy1JuvXWW9s7if3M0NBQXF+GZ/hd6WpK+flyHOiGckxZgVUKcFMguhQErpNmsxkV\nU3TVee5lOIP3AEVOONd4/1H7lVkdUj6P2tWunC6Gh4cjBNNqrpShBVbcseqM84BZBLQZ+pBy0ZCj\njjpKUq57Ohl+8jTGmAp48TTGmAq05baPj49Hci9dBe7icXdUypOU6WLTvuOOO8K++uqrw6a4hpQn\n3J944omSWu+qzRTUaKQ7QXeIroXUWguVCbsUCWERQblbyP8nhQPqFkuZPXt27HJzbnAucJdTyq8t\nxS0oCsFzXbZsWdjl+LJwgO096mZ4eDjcarbboA5l6ueTYKjiG9/4Rthsb8IePxTKoAsv5eOS5mDd\nu+28f5hNs3bt2rBTqDDBnXiGAjmneC9+6UtfCrssqmGBgvU8jTFmBvDiaYwxFWjrmb3RaIQbwZ1v\n1qNzF1SSVq1aFTa1HY8++uiwmVDOGufycZ079O3WoU4nyZ2ku0x3gkUEUr7TyiRuanNyHJkcTpdW\nynclkytX6h/ONJ2dnRFC+NWvfhXH2YaFx6W86IJzgAnRKVQjfVgkIU2sV2aieVn3Xidz586NjAi6\n2sw2uf/++7P3MPE76QVI+a4xMwp4L/Eek/Ld9hR2qztbZdasWXG9WPCStDUl6c4778ze06pzL0OJ\nqXBFys877agnWMSTwhp2240xZhrx4mmMMRVoy22fNWtWuBF0u1hfyl1TKd9Bo9xWq6599913X9il\nFBV32VL4YKoJrdMFXVS6m3TB6ZZJ+U7gxRdfHDbdFu4ss2VFuUvNDokpPFC3zNjo6GgkqjPJmy4i\nz1vKz4thB9q33XZb2F/5ylfCTq0tEnRdOTfrptFoRBYF9QoY7mEWgZTvOjM8ceqpp4bN2m+25yhb\nnVx33XVhn3/++ZLq757Z1dUVyfwMbzHhvZTW4z3PgghmdjzzzDNhM4xThvk4V/7zP/9TUh5emgw/\neRpjTAW8eBpjTAW8eBpjTAXainl2d3dHfIJtBGhPFkNhhQkrJCh2cckll4RdpvgwzpNinnVXSAwM\nDOi8886TlMdoGd875phjsvewioTCIIwPM8WGqRalkMOTTz4ZdhLbqHtM+vr6Ig7HODDj5BSAkfK0\nNKak8Tjjn4x7nXnmmdlnMY7F9Kbvf//7Uz+JaYatVdhe+fnnn89ex3NmWhoFd3i+jK+XAjHnnntu\n2CkFrG493B07doSwDtssn3766WEzLU3K7x+mY919991hsw0Oq/hKjWDGm4844ghJeYvsyfCTpzHG\nVMCLpzHGVKAt/27btm266667JOUiFnTNmPEv5ZUydMOZykPdQbaQKLUYWQ2QKhCY8lEHg4ODkXZD\nURS20SjFUphSQTeCVVusUKKLWroU7AyYqo/oBtZBb2+vFi1aJCl3HVN4Q5p4HtSqZNUV3UpqgHIu\nlRqNbNFR/q1O5syZo6VLl0rKQzlr1qwJuxR+YfUex4Lnz3DYd7/73bDTNUgwVJDSccrOrjNNb29v\n3OdMZWQ4qmzDwfQkhi+OO+64sJkOeNZZZ4XNVC4pv89SyltZsdYKP3kaY0wFvHgaY0wFOrjbu88X\nd3RslrRxny+cWRY3m80D9/2y6cFjMpGP6JhIHpe94TGZyJTGpK3F0xhjzAfYbTfGmAp48TTGmAp4\n8TTGmAp48TTGmAp48TTGmAp48TTGmAp48TTGmAp48TTGmAp48TTGmAr8P3hpvvFgwpNFAAAAAElF\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12aee5080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_conv_weights(weights, input_channel=0):\n",
    "    #use saved sess\n",
    "    model_path = os.path.expanduser(\"~/Desktop/best_model.ckpt\")\n",
    "    sess = tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, model_path)\n",
    "    \n",
    "    # Retrieve the values of the weight-variables from TensorFlow.\n",
    "    w = sess.run(weights)\n",
    "    # Get the lowest and highest values for the weights.\n",
    "    w_min = np.min(w)\n",
    "    w_max = np.max(w)\n",
    "    # Number of filters used in the conv. layer.\n",
    "    num_filters = w.shape[3]\n",
    "    # Number of grids to plot.\n",
    "    # Rounded-up, square-root of the number of filters.\n",
    "    num_grids = math.ceil(math.sqrt(num_filters))\n",
    "    \n",
    "    # Create figure with a grid of sub-plots.\n",
    "    fig, axes = plt.subplots(num_grids, num_grids)\n",
    "\n",
    "    # Plot all the filter-weights.\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Only plot the valid filter-weights.\n",
    "        if i<num_filters:\n",
    "            # Get the weights for the i'th filter of the input channel.\n",
    "            # See new_conv_layer() for details on the format\n",
    "            # of this 4-dim tensor.\n",
    "            img = w[:, :, input_channel, i]\n",
    "\n",
    "            # Plot image.\n",
    "            ax.imshow(img, vmin=w_min, vmax=w_max,\n",
    "                      interpolation='nearest', cmap=plt.cm.gray)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()\n",
    "\n",
    "#visualize the 25 filters in the first layer\n",
    "plot_conv_weights(W_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
